{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree | Assignment\n"
      ],
      "metadata": {
        "id": "Xl_d-m6uohpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm used for classification (categorical outcomes) and regression (continuous outcomes). It works by splitting data into subsets based on the values of input features, forming a tree-like structure of decisions.\n",
        "\n",
        "1.** Structure of a Decision Tree**\n",
        "\n",
        "A decision tree consists of:\n",
        "\n",
        "Root Node: Represents the entire dataset and the first decision point (feature to split on).\n",
        "\n",
        "Internal Nodes: Represent features used to split the data further.\n",
        "\n",
        "Branches: Represent the outcomes of the split.\n",
        "\n",
        "Leaf Nodes (Terminal Nodes): Represent the final classification or decision.\n",
        "\n",
        "----\n",
        "\n",
        "2. **How it works for Classification **\n",
        "\n",
        "The main goal is to classify data into categories by asking a series of yes/no (or multi-way) questions about features.\n",
        "  \n",
        "Step-by-step process:\n",
        "\n",
        "1. Select the Best Feature to Split\n",
        "\n",
        "   • Use a splitting criterion like:\n",
        "          \n",
        "          • Gini Impurity\n",
        "          • Entropy / Information Gain\n",
        "          • Chi-square\n",
        "\n",
        "Choose the feature that best separates the classes.\n",
        "\n",
        "2. Split the Data\n",
        "\n",
        "• Divide the dataset into subsets based on the selected feature’s values.\n",
        "\n",
        "3. Repeat Recursively\n",
        "\n",
        "• For each subset, repeat the process:\n",
        "\n",
        "     • Select the best feature.\n",
        "     • Split the subset.\n",
        "\n",
        "\n",
        "Continue until one of the stopping criteria is met:\n",
        "\n",
        "• All data points in a node belong to the same class.\n",
        "\n",
        "• Maximum tree depth is reached.\n",
        "\n",
        "• Minimum number of samples in a node is reached.\n",
        "\n",
        "\n",
        "4. Assign Class Labels\n",
        "\n",
        " Once a leaf node is reached, assign the most common class of that node to all data points in it.\n",
        "\n",
        "\n",
        "**3. Example**\n",
        "\n",
        "Suppose you want to classify if someone will play tennis based on weather:\n",
        "\n",
        "\n",
        "| Outlook  | Temperature | Humidity | Wind | Play Tennis |\n",
        "| -------- | ----------- | -------- | ---- | ----------- |\n",
        "| Sunny    | Hot         | High     | Weak | No          |\n",
        "| Overcast | Hot         | High     | Weak | Yes         |\n",
        "| Rain     | Mild        | High     | Weak | Yes         |\n",
        "\n",
        "\n",
        "The tree might first split on \"Outlook\":\n",
        "\n",
        "• If Sunny → split further by Humidity\n",
        "\n",
        "• If Rain → split by Wind\n",
        "\n",
        "• If Overcast → Yes (leaf node)\n",
        "\n",
        "\n",
        "**4. Advantages**\n",
        "\n",
        "• Easy to understand and interpret.\n",
        "\n",
        "• Can handle both numerical and categorical data.\n",
        "\n",
        "• Requires little data preprocessing.\n",
        "\n",
        "\n",
        "**5. Disadvantages**\n",
        "\n",
        "• Can overfit easily on training data.\n",
        "\n",
        "• Unstable to small variations in data.\n",
        "\n",
        "• Sometimes biased toward features with more levels.\n"
      ],
      "metadata": {
        "id": "a5fMHX_Govj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "**Decision Trees & Impurity**\n",
        "\n",
        "When building a Decision Tree, the goal at each split is to separate the data into “purer” groups — meaning each node should ideally contain samples from only one class.\n",
        "\n",
        "To measure this impurity (or disorder) of a node, we commonly use:\n",
        "\n",
        "1. Gini Impurity\n",
        "\n",
        "2. Entropy (Information Gain)\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "1. Gini Impurity\n",
        "\n",
        "• Formula:\n",
        "\n",
        "**Gini Impurity Formula:**\n",
        "\n",
        "$$\n",
        "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "$$\n",
        "\n",
        "\n",
        "Meaning:\n",
        "Gini measures how often you would misclassify a sample if you randomly labeled it according to the class distribution in the node.\n",
        "\n",
        "Range:\n",
        "\n",
        "\n",
        "0 → Pure (all samples belong to one class)\n",
        "\n",
        "Maximum ≈  0.5 (for 2 classes with 50-50 distribution)\n",
        "\n",
        "\n",
        "Example: If a node has 70% Class A and 30% Class B\n",
        "\n",
        "**Example: Gini Impurity Calculation**\n",
        "\n",
        "$$\n",
        "Gini = 1 - (0.7^2 + 0.3^2) = 1 - (0.49 + 0.09) = 0.42\n",
        "$$\n",
        "\n",
        "--------\n",
        "\n",
        "**2. Entropy (Information Gain)**\n",
        "\n",
        "• Formula:\n",
        "\n",
        "**Entropy Formula:**\n",
        "\n",
        "$$\n",
        "Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "\n",
        "**Meaning:**\n",
        "\n",
        "Entropy measures the amount of uncertainty or “disorder” in the node.\n",
        "The more evenly distributed the classes, the higher the entropy.\n",
        "\n",
        "**Range:**\n",
        "\n",
        "0 → Pure (all samples same class)\n",
        "\n",
        "1 → Maximum impurity (for 2 classes split 50-50)\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Same node: 70% Class A, 30% Class B:\n",
        "\n",
        "**Example: Entropy Calculation**\n",
        "\n",
        "$$\n",
        "Entropy = -(0.7 \\log_{2} 0.7 + 0.3 \\log_{2} 0.3) \\approx 0.88\n",
        "$$\n",
        "\n",
        "\n",
        "**How They Impact Splits in Decision Trees**\n",
        "\n",
        "Both Gini and Entropy are used to choose the best split:\n",
        "\n",
        " • The algorithm checks all possible splits.\n",
        "\n",
        "• For each split, it calculates the impurity of the child nodes.\n",
        "\n",
        "• It then picks the split that reduces impurity the most (highest information gain for entropy, or highest Gini decrease for Gini).\n",
        "\n",
        "\n",
        "**Differences in practice:**\n",
        "\n",
        "• Gini is computationally simpler (no log function) → slightly faster.\n",
        "\n",
        "• Entropy tends to give more balanced splits (more sensitive to class distribution).\n",
        "\n",
        "• In practice, both usually give very similar trees."
      ],
      "metadata": {
        "id": "8JhqnUTYAAgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "**Pre-Pruning (Early Stopping)**\n",
        "\n",
        "• **Definition:** The tree growth is stopped early, before it becomes too complex.\n",
        "\n",
        "• **How:** Uses constraints like maximum depth, minimum samples per split, or minimum information gain.\n",
        "\n",
        "• **Goal:** Prevents overfitting by not letting the tree grow unnecessarily deep.\n",
        "\n",
        "**Practical Advantage**: Saves training time and computation, especially useful for large datasets where building a very deep tree would be slow and expensive.\n",
        "\n",
        "--------\n",
        "\n",
        "**Post-Pruning (Pruning after Full Growth)**\n",
        "\n",
        "• **Definition:** The tree is first allowed to grow fully, and then unnecessary branches are cut back.\n",
        "\n",
        "• **How:** Uses techniques like cost-complexity pruning (CART) or reduced error pruning to remove branches that don’t improve accuracy.\n",
        "\n",
        "• **Goal:** Simplifies the model while keeping performance high.\n",
        "\n",
        "\n",
        "**Practical Advantage:** Produces a more accurate and generalizable model, since pruning decisions are made after seeing the complete tree.\n",
        "\n"
      ],
      "metadata": {
        "id": "tgOOBdDjI2G3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "**Information Gain (IG)**\n",
        "\n",
        "**Definition:** Information Gain measures how much \"uncertainty\" (impurity) in the dataset is reduced after splitting on a feature.\n",
        "\n",
        "Formula:\n",
        "\n",
        "$$\n",
        "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v)\n",
        "$$\n",
        "\n",
        "**Why It’s Important**\n",
        "\n",
        "• A Decision Tree works by splitting data into purer subsets (where classes are more separated).\n",
        "\n",
        "• Information Gain tells us which feature is the “best question” to ask at each step.\n",
        "\n",
        "• Higher IG = bigger reduction in impurity = better split."
      ],
      "metadata": {
        "id": "PZK8W_-INadh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "**Real-World Applications of Decision Trees**\n",
        "\n",
        "\n",
        " • **Medical diagnosis** – Predicting if a patient has a disease based on symptoms and test results.\n",
        "\n",
        " • **Credit scoring –** Classifying loan applicants as “low risk” or “high risk based on financial history.\n",
        "\n",
        "• **Fraud detection –** Identifying suspicious transactions.\n",
        "\n",
        "• **Customer churn prediction** – Predicting if a customer will stop using service.\n",
        "\n",
        "• **Product recommendation** – Suggesting products based on user behavior and preferences.\n",
        "\n",
        "• **Regression tasks **– Predicting house prices, sales forecasting, etc.\n",
        "\n",
        "---------\n",
        "\n",
        "**Main advantages:**\n",
        "\n",
        "\n",
        "• **Easy to understand & interpret**   – Produces clear, human-readable rules.\n",
        "\n",
        "• **Handles both numerical and categorical data**  – Works with mixed data types without complex preprocessing.\n",
        "\n",
        "• **No need for feature scaling** – Normalization or standardization is not required.\n",
        "\n",
        "• **Captures non-linear relationships** – Can model complex decision boundaries.\n",
        "\n",
        "• **Feature importance** – Identifies which variables are most influential.\n",
        "\n",
        "-------\n",
        "\n",
        "**Main limitations:**\n",
        "\n",
        "\n",
        "• **Overfitting** – Fully grown trees can memorize the training data and perform poorly on unseen data.\n",
        "\n",
        "• **High variance** – Small changes in data can produce very different trees.\n",
        "\n",
        "\n",
        "• **Bias toward features with many levels**  – Features with more unique values can dominate splits.\n",
        "\n",
        "• **Less accurate than ensembles** – Often outperformed by Random Forests or Gradient Boosting on complex problems.\n"
      ],
      "metadata": {
        "id": "IKz-DqtggGFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split into training and test sets (stratified to maintain class proportions)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Output\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Feature importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"  {name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z863yCH9jtpw",
        "outputId": "de77ae4d-1e4a-41e1-a7b7-0a668b2d433e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333\n",
            "Feature importances:\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0286\n",
            "  petal length (cm): 0.5412\n",
            "  petal width (cm): 0.4303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train/test sets (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Fully-grown tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "# Tree with max_depth=3\n",
        "clf_md3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_md3.fit(X_train, y_train)\n",
        "acc_md3 = accuracy_score(y_test, clf_md3.predict(X_test))\n",
        "\n",
        "# Output\n",
        "print(f\"Fully-grown tree accuracy: {acc_full:.4f}\")\n",
        "print(f\"Max depth=3 tree accuracy: {acc_md3:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7NQItvZkFWi",
        "outputId": "41beca2b-825d-415e-824f-60f1a16448f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown tree accuracy: 0.9333\n",
            "Max depth=3 tree accuracy: 0.9778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a Python program to:\n",
        "# ● Load the California Housing dataset from sklearn\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Squared Error on test data: {mse:.4f}\\n\")\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\" - {name}: {importance:.4f}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FahOcvVJkeg5",
        "outputId": "6d63521f-099c-43db-c6fd-e7a210a92fb3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on test data: 0.4952\n",
            "\n",
            "Feature Importances:\n",
            " - MedInc: 0.5285\n",
            " - HouseAge: 0.0519\n",
            " - AveRooms: 0.0530\n",
            " - AveBedrms: 0.0287\n",
            " - Population: 0.0305\n",
            " - AveOccup: 0.1308\n",
            " - Latitude: 0.0937\n",
            " - Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up the grid of parameters to search\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearch to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Evaluate the best estimator on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# Print the results\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Model accuracy on test set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y3NVatIkwMD",
        "outputId": "0dd65bf3-4490-403b-fc84-ea3d55aa1cff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model accuracy on test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "\n",
        "---\n",
        "**bold text**\n",
        "\n",
        "Answer:\n",
        "\n",
        "**1. Handle Missing Values**\n",
        "\n",
        "**Understand the missingness:**\n",
        "\n",
        "First, analyze how data is missing. Is it random, or does it follow a pattern? This affects how you handle it.\n",
        "\n",
        "• **Imputation:**\n",
        "\n",
        "For **numerical features**, you could fill missing values with mean, median, or use more advanced methods like K-Nearest Neighbors imputation.\n",
        "\n",
        "For **categorical features**, you might fill missing with the mode (most frequent value) or create a special category like \"Unknown\".\n",
        "\n",
        "If missing values are too prevalent or critical, consider dropping those features or samples carefully.\n",
        "\n",
        "**Why it matters:** Models can’t handle missing data directly, so cleaning this up ensures your model sees complete, reliable inputs.\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "**2. Encode Categorical Features**\n",
        "\n",
        "• **Identify categorical variables:** This could be patient gender, blood type, or any non-numeric info.\n",
        "\n",
        "• **Encoding methods:**\n",
        "\n",
        "• For **nominal categories** without order (e.g., blood type), use One-Hot Encoding.\n",
        "\n",
        "• For **ordinal categories** (e.g., disease severity: mild, moderate, severe), use Label Encoding or map them to meaningful numeric scales.\n",
        "\n",
        "• **Why it matters:** Machine learning models, including Decision Trees, require numeric input, so encoding transforms your data into a digestible form.\n",
        "\n",
        "------\n",
        "\n",
        "**3. Train a Decision Tree Model**\n",
        "\n",
        "• **Split the data**: Use an 80-20 or 70-30 split between training and test sets, or use cross-validation to ensure your results generalize.\n",
        "\n",
        "• **Initialize the model**: Start with a default Decision Tree classifier.\n",
        "\n",
        "• **Train on processed data**: Fit the model on your training data.\n",
        "\n",
        "• **Why Decision Trees**: They handle mixed data types well, are interpretable (important in healthcare), and can capture nonlinear patterns\n",
        "\n",
        "--------\n",
        "\n",
        "**4. Tune Hyperparameters**\n",
        "\n",
        "• **Key hyperparameters to tune:**\n",
        "    \n",
        "   • max_depth: controls tree complexity, balancing underfitting and overfitting.\n",
        "\n",
        "   • min_samples_split and min_samples_leaf: control how many samples needed to split or be a leaf node, affecting generalization.\n",
        "\n",
        "   • max_features: number of features to consider at each split.\n",
        "\n",
        "• **Use GridSearchCV or RandomizedSearchCV**: Explore combinations systematically with cross-validation to find the sweet spot.\n",
        "\n",
        "• **Why tuning matters**: Proper tuning prevents overfitting or underfitting, improving the model’s predictive power on unseen data.\n",
        "\n",
        "\n",
        "-------------\n",
        "\n",
        "**5. Evaluate Performance**\n",
        "\n",
        "**Metrics:**\n",
        "\n",
        "For classification, consider Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n",
        "\n",
        "In healthcare, Recall (sensitivity) is often critical — you want to catch as many patients with the disease as possible, even at the cost of some false positives.\n",
        "\n",
        "**Validation**: Use a separate test set or cross-validation to ensure your model performs reliably.\n",
        "\n",
        "**Interpretability**: Use feature importance and decision tree visualization to explain model decisions to clinicians and stakeholders.\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "\n",
        "**Business Value of This Model**\n",
        "\n",
        "**Early detection**: Predicting disease early means patients can receive timely treatment, improving outcomes and reducing healthcare costs.\n",
        "\n",
        "**Resource optimization**: Helps healthcare providers prioritize high-risk patients for screening or intervention, making better use of limited resources.\n",
        "\n",
        "**Personalized care**: Tailors monitoring and care plans based on individual risk, improving patient satisfaction and effectiveness.\n",
        "\n",
        "**Data-driven decisions**: Provides actionable insights backed by data, enabling the company to develop better products, policies, or outreach programs.\n",
        "\n",
        "**Trust and transparency**: Decision Trees’ interpretability supports building trust with clinicians, regulators, and patients, crucial in healthcare.\n"
      ],
      "metadata": {
        "id": "-iSBl1RclVvq"
      }
    }
  ]
}